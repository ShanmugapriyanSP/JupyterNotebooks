{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = -1\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:                \n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)        \n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n",
    "        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../files/Kaggle Competition/House Prediction/final_data_cleaned_dummies.csv',encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 90.01 MB --> 19.16 MB (Decreased by 78.7%)\n"
     ]
    }
   ],
   "source": [
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=10, min_child_weight=1, missing=None, n_estimators=200,\n",
       "             n_jobs=-1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xg = XGBRegressor(n_estimators=200, n_jobs=-1, objective='reg:squarederror', max_depth=10)\n",
    "xg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9337570505549619"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9273423361227081"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k-fold Cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = xg, X = x_train, y = y_train, cv = 10, n_jobs=-1)\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  30 out of  30 | elapsed: 25.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:31:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.9243505651953151\n",
      "{'xgbregressor': XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=10, min_child_weight=1, missing=None, n_estimators=200,\n",
      "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "             silent=None, subsample=1, verbosity=1), 'xgbregressor__max_depth': 10, 'xgbregressor__n_estimators': 200}\n",
      "Pipeline(memory=None,\n",
      "         steps=[('xgbregressor',\n",
      "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
      "                              colsample_bylevel=1, colsample_bynode=1,\n",
      "                              colsample_bytree=1, gamma=0,\n",
      "                              importance_type='gain', learning_rate=0.1,\n",
      "                              max_delta_step=0, max_depth=10,\n",
      "                              min_child_weight=1, missing=None,\n",
      "                              n_estimators=200, n_jobs=1, nthread=None,\n",
      "                              objective='reg:linear', random_state=0,\n",
      "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                              seed=None, silent=None, subsample=1,\n",
      "                              verbosity=1))],\n",
      "         verbose=False)\n",
      "Wall time: 28min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a pipeline\n",
    "pipe = make_pipeline((XGBRegressor()))\n",
    "# Create dictionary with candidate learning algorithms and their hyperparameters\n",
    "grid_param = [\n",
    "                {\"xgbregressor\": [XGBRegressor()],\n",
    "                 \"xgbregressor__n_estimators\": [100,200],\n",
    "                 \"xgbregressor__max_depth\":[10,15,20]                 \n",
    "                 }]\n",
    "# create a gridsearch of the pipeline, the fit the best model\n",
    "gridsearch = GridSearchCV(pipe, grid_param, cv=10, verbose=2,n_jobs=7) # Fit grid search\n",
    "best_model = gridsearch.fit(x_train,y_train)\n",
    "print(best_model.best_score_)\n",
    "print(best_model.best_params_)\n",
    "print(best_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalized parameters\n",
    "\n",
    "n_estimators = 200, max_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xg, open('xgboost_model.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
